
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_tutorial/plot_usparse_xgboost.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_tutorial_plot_usparse_xgboost.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_tutorial_plot_usparse_xgboost.py:


.. _example-sparse-tfidf:

TfIdf and sparse matrices
=========================

.. index:: xgboost, lightgbm, sparse, ensemble

`TfidfVectorizer <https://scikit-learn.org/stable/modules/
generated/sklearn.feature_extraction.text.TfidfVectorizer.html>`_
usually creates sparse data. If the data is sparse enough, matrices
usually stays as sparse all along the pipeline until the predictor
is trained. Sparse matrices do not consider null and missing values
as they are not present in the datasets. Because some predictors
do the difference, this ambiguity may introduces discrepencies
when converter into ONNX. This example looks into several configurations.

Imports, setups
+++++++++++++++

All imports. It also registered onnx converters for :epkg:`xgboost`
and *lightgbm*.

.. GENERATED FROM PYTHON SOURCE LINES 26-69

.. code-block:: Python


    import warnings
    import numpy
    import pandas
    import onnxruntime as rt
    from tqdm import tqdm
    from sklearn.compose import ColumnTransformer
    from sklearn.datasets import load_iris
    from sklearn.pipeline import Pipeline
    from sklearn.preprocessing import StandardScaler
    from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
    from sklearn.ensemble import RandomForestClassifier

    try:
        from sklearn.ensemble import HistGradientBoostingClassifier
    except ImportError:
        HistGradientBoostingClassifier = None
    from xgboost import XGBClassifier
    from lightgbm import LGBMClassifier
    from skl2onnx.common.data_types import FloatTensorType, StringTensorType
    from skl2onnx import to_onnx, update_registered_converter
    from skl2onnx.sklapi import CastTransformer, ReplaceTransformer
    from skl2onnx.common.shape_calculator import calculate_linear_classifier_output_shapes
    from onnxmltools.convert.xgboost.operator_converters.XGBoost import convert_xgboost
    from onnxmltools.convert.lightgbm.operator_converters.LightGbm import convert_lightgbm


    update_registered_converter(
        XGBClassifier,
        "XGBoostXGBClassifier",
        calculate_linear_classifier_output_shapes,
        convert_xgboost,
        options={"nocl": [True, False], "zipmap": [True, False, "columns"]},
    )
    update_registered_converter(
        LGBMClassifier,
        "LightGbmLGBMClassifier",
        calculate_linear_classifier_output_shapes,
        convert_lightgbm,
        options={"nocl": [True, False], "zipmap": [True, False]},
    )









.. GENERATED FROM PYTHON SOURCE LINES 70-74

Artificial datasets
+++++++++++++++++++++++++++

Iris + a text column.

.. GENERATED FROM PYTHON SOURCE LINES 74-92

.. code-block:: Python


    cst = ["class zero", "class one", "class two"]

    data = load_iris()
    X = data.data[:, :2]
    y = data.target

    df = pandas.DataFrame(X)
    df.columns = [f"c{c}" for c in df.columns]
    df["text"] = [cst[i] for i in y]


    ind = numpy.arange(X.shape[0])
    numpy.random.shuffle(ind)
    X = X[ind, :].copy()
    y = y[ind].copy()









.. GENERATED FROM PYTHON SOURCE LINES 93-99

Train ensemble after sparse
+++++++++++++++++++++++++++

The example use the Iris datasets with artifical text datasets
preprocessed with a tf-idf. `sparse_threshold=1.` avoids
sparse matrices to be converted into dense matrices.

.. GENERATED FROM PYTHON SOURCE LINES 99-233

.. code-block:: Python



    def make_pipelines(
        df_train,
        y_train,
        models=None,
        sparse_threshold=1.0,
        replace_nan=False,
        insert_replace=False,
    ):
        if models is None:
            models = [
                RandomForestClassifier,
                HistGradientBoostingClassifier,
                XGBClassifier,
                LGBMClassifier,
            ]
        models = [_ for _ in models if _ is not None]

        pipes = []
        for model in tqdm(models):
            if model == HistGradientBoostingClassifier:
                kwargs = dict(max_iter=5)
            elif model == XGBClassifier:
                kwargs = dict(n_estimators=5, use_label_encoder=False)
            else:
                kwargs = dict(n_estimators=5)

            if insert_replace:
                pipe = Pipeline(
                    [
                        (
                            "union",
                            ColumnTransformer(
                                [
                                    ("scale1", StandardScaler(), [0, 1]),
                                    (
                                        "subject",
                                        Pipeline(
                                            [
                                                ("count", CountVectorizer()),
                                                ("tfidf", TfidfTransformer()),
                                                ("repl", ReplaceTransformer()),
                                            ]
                                        ),
                                        "text",
                                    ),
                                ],
                                sparse_threshold=sparse_threshold,
                            ),
                        ),
                        ("cast", CastTransformer()),
                        ("cls", model(max_depth=3, **kwargs)),
                    ]
                )
            else:
                pipe = Pipeline(
                    [
                        (
                            "union",
                            ColumnTransformer(
                                [
                                    ("scale1", StandardScaler(), [0, 1]),
                                    (
                                        "subject",
                                        Pipeline(
                                            [
                                                ("count", CountVectorizer()),
                                                ("tfidf", TfidfTransformer()),
                                            ]
                                        ),
                                        "text",
                                    ),
                                ],
                                sparse_threshold=sparse_threshold,
                            ),
                        ),
                        ("cast", CastTransformer()),
                        ("cls", model(max_depth=3, **kwargs)),
                    ]
                )

            try:
                pipe.fit(df_train, y_train)
            except TypeError as e:
                obs = dict(model=model.__name__, pipe=pipe, error=e, model_onnx=None)
                pipes.append(obs)
                continue

            options = {model: {"zipmap": False}}
            if replace_nan:
                options[TfidfTransformer] = {"nan": True}

            # convert
            with warnings.catch_warnings(record=False):
                warnings.simplefilter("ignore", (FutureWarning, UserWarning))
                model_onnx = to_onnx(
                    pipe,
                    initial_types=[
                        ("input", FloatTensorType([None, 2])),
                        ("text", StringTensorType([None, 1])),
                    ],
                    target_opset={"": 12, "ai.onnx.ml": 2},
                    options=options,
                )

            with open("model.onnx", "wb") as f:
                f.write(model_onnx.SerializeToString())

            sess = rt.InferenceSession(
                model_onnx.SerializeToString(), providers=["CPUExecutionProvider"]
            )
            inputs = {
                "input": df[["c0", "c1"]].values.astype(numpy.float32),
                "text": df[["text"]].values,
            }
            pred_onx = sess.run(None, inputs)

            diff = numpy.abs(pred_onx[1].ravel() - pipe.predict_proba(df).ravel()).sum()

            obs = dict(
                model=model.__name__, discrepencies=diff, model_onnx=model_onnx, pipe=pipe
            )
            pipes.append(obs)

        return pipes


    data_sparse = make_pipelines(df, y)
    stat = pandas.DataFrame(data_sparse).drop(["model_onnx", "pipe"], axis=1)
    if "error" in stat.columns:
        print(stat.drop("error", axis=1))
    stat





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/4 [00:00<?, ?it/s]/home/xadupre/vv/this312/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [09:45:35] WARNING: /workspace/src/learner.cc:738: 
    Parameters: { "use_label_encoder" } are not used.

      bst.update(dtrain, iteration=i, fobj=obj)
     75%|███████▌  | 3/4 [00:00<00:00,  3.18it/s][LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000728 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 53
    [LightGBM] [Info] Number of data points in the train set: 150, number of used features: 5
    [LightGBM] [Info] Start training from score -1.098612
    [LightGBM] [Info] Start training from score -1.098612
    [LightGBM] [Info] Start training from score -1.098612
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    /home/xadupre/vv/this312/lib/python3.12/site-packages/sklearn/utils/validation.py:2735: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names
      warnings.warn(
    100%|██████████| 4/4 [00:01<00:00,  3.84it/s]
                                model  discrepencies
    0          RandomForestClassifier       0.947052
    1  HistGradientBoostingClassifier            NaN
    2                   XGBClassifier      15.196619
    3                  LGBMClassifier       0.000009


.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>model</th>
          <th>discrepencies</th>
          <th>error</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>RandomForestClassifier</td>
          <td>0.947052</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>1</th>
          <td>HistGradientBoostingClassifier</td>
          <td>NaN</td>
          <td>Sparse data was passed for X, but dense data i...</td>
        </tr>
        <tr>
          <th>2</th>
          <td>XGBClassifier</td>
          <td>15.196619</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>3</th>
          <td>LGBMClassifier</td>
          <td>0.000009</td>
          <td>NaN</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 234-240

Sparse data hurts.

Dense data
++++++++++

Let's replace sparse data with dense by using `sparse_threshold=0.`

.. GENERATED FROM PYTHON SOURCE LINES 240-248

.. code-block:: Python



    data_dense = make_pipelines(df, y, sparse_threshold=0.0)
    stat = pandas.DataFrame(data_dense).drop(["model_onnx", "pipe"], axis=1)
    if "error" in stat.columns:
        print(stat.drop("error", axis=1))
    stat





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/4 [00:00<?, ?it/s]     50%|█████     | 2/4 [00:00<00:00,  2.78it/s]/home/xadupre/vv/this312/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [09:45:37] WARNING: /workspace/src/learner.cc:738: 
    Parameters: { "use_label_encoder" } are not used.

      bst.update(dtrain, iteration=i, fobj=obj)
     75%|███████▌  | 3/4 [00:01<00:00,  1.81it/s][LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033149 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 53
    [LightGBM] [Info] Number of data points in the train set: 150, number of used features: 5
    [LightGBM] [Info] Start training from score -1.098612
    [LightGBM] [Info] Start training from score -1.098612
    [LightGBM] [Info] Start training from score -1.098612
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    /home/xadupre/vv/this312/lib/python3.12/site-packages/sklearn/utils/validation.py:2735: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names
      warnings.warn(
    100%|██████████| 4/4 [00:01<00:00,  2.58it/s]    100%|██████████| 4/4 [00:01<00:00,  2.42it/s]


.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>model</th>
          <th>discrepencies</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>RandomForestClassifier</td>
          <td>0.940913</td>
        </tr>
        <tr>
          <th>1</th>
          <td>HistGradientBoostingClassifier</td>
          <td>0.000005</td>
        </tr>
        <tr>
          <th>2</th>
          <td>XGBClassifier</td>
          <td>2.899390</td>
        </tr>
        <tr>
          <th>3</th>
          <td>LGBMClassifier</td>
          <td>0.000009</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 249-251

This is much better. Let's compare how the preprocessing
applies on the data.

.. GENERATED FROM PYTHON SOURCE LINES 251-258

.. code-block:: Python


    print("sparse")
    print(data_sparse[-1]["pipe"].steps[0][-1].transform(df)[:2])
    print()
    print("dense")
    print(data_dense[-1]["pipe"].steps[0][-1].transform(df)[:2])





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    sparse
    <Compressed Sparse Row sparse matrix of dtype 'float64'
            with 8 stored elements and shape (2, 6)>
      Coords        Values
      (0, 0)        -0.9006811702978088
      (0, 1)        1.019004351971607
      (0, 2)        0.4323732931220851
      (0, 5)        0.9016947018779491
      (1, 0)        -1.1430169111851105
      (1, 1)        -0.13197947932162468
      (1, 2)        0.4323732931220851
      (1, 5)        0.9016947018779491

    dense
    [[-0.90068117  1.01900435  0.43237329  0.          0.          0.9016947 ]
     [-1.14301691 -0.13197948  0.43237329  0.          0.          0.9016947 ]]




.. GENERATED FROM PYTHON SOURCE LINES 259-278

This shows `RandomForestClassifier
<https://scikit-learn.org/stable/modules/generated/
sklearn.ensemble.RandomForestClassifier.html>`_,
`XGBClassifier <https://xgboost.readthedocs.io/
en/latest/python/python_api.html>`_ do not process
the same way sparse and
dense matrix as opposed to `LGBMClassifier
<https://lightgbm.readthedocs.io/en/latest/
pythonapi/lightgbm.LGBMClassifier.html>`_.
And `HistGradientBoostingClassifier
<https://scikit-learn.org/stable/modules/generated/
sklearn.ensemble.HistGradientBoostingClassifier.html>`_
fails.

Dense data with nan
+++++++++++++++++++

Let's keep sparse data in the scikit-learn pipeline but
replace null values by nan in the onnx graph.

.. GENERATED FROM PYTHON SOURCE LINES 278-286

.. code-block:: Python


    data_dense = make_pipelines(df, y, sparse_threshold=1.0, replace_nan=True)
    stat = pandas.DataFrame(data_dense).drop(["model_onnx", "pipe"], axis=1)
    if "error" in stat.columns:
        print(stat.drop("error", axis=1))
    stat






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/4 [00:00<?, ?it/s]/home/xadupre/vv/this312/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [09:45:38] WARNING: /workspace/src/learner.cc:738: 
    Parameters: { "use_label_encoder" } are not used.

      bst.update(dtrain, iteration=i, fobj=obj)
     75%|███████▌  | 3/4 [00:00<00:00,  3.21it/s][LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012050 seconds.
    You can set `force_row_wise=true` to remove the overhead.
    And if memory is not enough, you can set `force_col_wise=true`.
    [LightGBM] [Info] Total Bins 53
    [LightGBM] [Info] Number of data points in the train set: 150, number of used features: 5
    [LightGBM] [Info] Start training from score -1.098612
    [LightGBM] [Info] Start training from score -1.098612
    [LightGBM] [Info] Start training from score -1.098612
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    /home/xadupre/vv/this312/lib/python3.12/site-packages/sklearn/utils/validation.py:2735: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names
      warnings.warn(
    100%|██████████| 4/4 [00:01<00:00,  4.06it/s]    100%|██████████| 4/4 [00:01<00:00,  3.83it/s]
                                model  discrepencies
    0          RandomForestClassifier      24.634892
    1  HistGradientBoostingClassifier            NaN
    2                   XGBClassifier       2.899390
    3                  LGBMClassifier       0.000009


.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>model</th>
          <th>discrepencies</th>
          <th>error</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>RandomForestClassifier</td>
          <td>24.634892</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>1</th>
          <td>HistGradientBoostingClassifier</td>
          <td>NaN</td>
          <td>Sparse data was passed for X, but dense data i...</td>
        </tr>
        <tr>
          <th>2</th>
          <td>XGBClassifier</td>
          <td>2.899390</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>3</th>
          <td>LGBMClassifier</td>
          <td>0.000009</td>
          <td>NaN</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 287-296

Dense, 0 replaced by nan
++++++++++++++++++++++++

Instead of using a specific options to replace null values
into nan values, a custom transformer called
ReplaceTransformer is explicitely inserted into the pipeline.
A new converter is added to the list of supported models.
It is equivalent to the previous options except it is
more explicit.

.. GENERATED FROM PYTHON SOURCE LINES 296-305

.. code-block:: Python


    data_dense = make_pipelines(
        df, y, sparse_threshold=1.0, replace_nan=False, insert_replace=True
    )
    stat = pandas.DataFrame(data_dense).drop(["model_onnx", "pipe"], axis=1)
    if "error" in stat.columns:
        print(stat.drop("error", axis=1))
    stat





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/4 [00:00<?, ?it/s]/home/xadupre/vv/this312/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [09:45:39] WARNING: /workspace/src/learner.cc:738: 
    Parameters: { "use_label_encoder" } are not used.

      bst.update(dtrain, iteration=i, fobj=obj)
     75%|███████▌  | 3/4 [00:01<00:00,  2.67it/s][LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000045 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 53
    [LightGBM] [Info] Number of data points in the train set: 150, number of used features: 5
    [LightGBM] [Info] Start training from score -1.098612
    [LightGBM] [Info] Start training from score -1.098612
    [LightGBM] [Info] Start training from score -1.098612
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    /home/xadupre/vv/this312/lib/python3.12/site-packages/sklearn/utils/validation.py:2735: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names
      warnings.warn(
    100%|██████████| 4/4 [00:01<00:00,  3.30it/s]
                                model  discrepencies
    0          RandomForestClassifier      41.288296
    1  HistGradientBoostingClassifier            NaN
    2                   XGBClassifier       2.899390
    3                  LGBMClassifier       0.000009


.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>model</th>
          <th>discrepencies</th>
          <th>error</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>RandomForestClassifier</td>
          <td>41.288296</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>1</th>
          <td>HistGradientBoostingClassifier</td>
          <td>NaN</td>
          <td>Sparse data was passed for X, but dense data i...</td>
        </tr>
        <tr>
          <th>2</th>
          <td>XGBClassifier</td>
          <td>2.899390</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>3</th>
          <td>LGBMClassifier</td>
          <td>0.000009</td>
          <td>NaN</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 306-312

Conclusion
++++++++++

Unless dense arrays are used, because *onnxruntime*
ONNX does not support sparse yet, the conversion needs to be
tuned depending on the model which follows the TfIdf preprocessing.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 4.999 seconds)


.. _sphx_glr_download_auto_tutorial_plot_usparse_xgboost.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_usparse_xgboost.ipynb <plot_usparse_xgboost.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_usparse_xgboost.py <plot_usparse_xgboost.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_usparse_xgboost.zip <plot_usparse_xgboost.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
