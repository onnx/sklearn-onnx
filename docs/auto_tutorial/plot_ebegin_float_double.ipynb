{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Issues when switching to float\n\n.. index:: float, double, discrepencies\n\nMost models in :epkg:`scikit-learn` do computation with double,\nnot float. Most models in deep learning use float because\nthat's the most common situation with GPU. ONNX was initially\ncreated to facilitate the deployment of deep learning models\nand that explains why many converters assume the converted models\nshould use float. That assumption does not usually harm\nthe predictions, the conversion to float introduce small\ndiscrepencies compare to double predictions.\nThat assumption is usually true if the prediction\nfunction is continuous, $y = f(x)$, then\n$dy = f'(x) dx$. We can determine an upper bound\nto the discrepencies :\n$\\Delta(y) \\leqslant \\sup_x \\left\\Vert f'(x)\\right\\Vert dx$.\n*dx* is the discrepency introduced by a float conversion,\n``dx = x - numpy.float32(x)``.\n\nHowever, that's not the case for every model. A decision tree\ntrained for a regression is not a continuous function. Therefore,\neven a small *dx* may introduce a huge discrepency. Let's look into\nan example which always produces discrepencies and some ways\nto overcome this situation.\n\n## More into the issue\n\nThe below example is built to fail.\nIt contains integer features with different order\nof magnitude rounded to integer. A decision tree compares\nfeatures to thresholds. In most cases, float and double\ncomparison gives the same result. We denote\n$[x]_{f32}$ the conversion (or cast)\n``numpy.float32(x)``.\n\n\\begin{align}x \\leqslant y = [x]_{f32} \\leqslant [y]_{f32}\\end{align}\n\nHowever, the probability that both comparisons give\ndifferent results is not null. The following graph shows\nthe discord areas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mlprodict.sklapi import OnnxPipeline\nfrom skl2onnx.sklapi import CastTransformer, CastRegressor\nfrom skl2onnx import to_onnx\nfrom mlprodict.onnx_conv import to_onnx as to_onnx_extended\nfrom mlprodict.onnxrt import OnnxInference\nfrom onnxruntime import InferenceSession\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.datasets import make_regression\nimport numpy\nimport matplotlib.pyplot as plt\n\n\ndef area_mismatch_rule(N, delta, factor, rule=None):\n    if rule is None:\n        def rule(t): return numpy.float32(t)\n    xst = []\n    yst = []\n    xsf = []\n    ysf = []\n    for x in range(-N, N):\n        for y in range(-N, N):\n            dx = (1. + x * delta) * factor\n            dy = (1. + y * delta) * factor\n            c1 = 1 if numpy.float64(dx) <= numpy.float64(dy) else 0\n            c2 = 1 if numpy.float32(dx) <= rule(dy) else 0\n            key = abs(c1 - c2)\n            if key == 1:\n                xsf.append(dx)\n                ysf.append(dy)\n            else:\n                xst.append(dx)\n                yst.append(dy)\n    return xst, yst, xsf, ysf\n\n\ndelta = 36e-10\nfactor = 1\nxst, yst, xsf, ysf = area_mismatch_rule(100, delta, factor)\n\n\nfig, ax = plt.subplots(1, 1, figsize=(5, 5))\nax.plot(xst, yst, '.', label=\"agree\")\nax.plot(xsf, ysf, '.', label=\"disagree\")\nax.set_title(\"Region where x <= y and (float)x <= (float)y agree\")\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y\")\nax.plot([min(xst), max(xst)], [min(yst), max(yst)], 'k--')\nax.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The pipeline and the data\n\nWe can now build an example where the learned decision tree\ndoes many comparisons in this discord area. This is done\nby rounding features to integers, a frequent case\nhappening when dealing with categorical features.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y = make_regression(10000, 10)\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\nXi_train, yi_train = X_train.copy(), y_train.copy()\nXi_test, yi_test = X_test.copy(), y_test.copy()\nfor i in range(X.shape[1]):\n    Xi_train[:, i] = (Xi_train[:, i] * 2 ** i).astype(numpy.int64)\n    Xi_test[:, i] = (Xi_test[:, i] * 2 ** i).astype(numpy.int64)\n\nmax_depth = 10\n\nmodel = Pipeline([\n    ('scaler', StandardScaler()),\n    ('dt', DecisionTreeRegressor(max_depth=max_depth))\n])\n\nmodel.fit(Xi_train, yi_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The discrepencies\n\nLet's reuse the function implemented in the\nfirst example `l-diff-dicrepencies` and\nlook into the conversion.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def diff(p1, p2):\n    p1 = p1.ravel()\n    p2 = p2.ravel()\n    d = numpy.abs(p2 - p1)\n    return d.max(), (d / numpy.abs(p1)).max()\n\n\nonx = to_onnx(model, Xi_train[:1].astype(numpy.float32))\n\nsess = InferenceSession(onx.SerializeToString())\n\nX32 = Xi_test.astype(numpy.float32)\n\nskl = model.predict(X32)\nort = sess.run(None, {'X': X32})[0]\n\nprint(diff(skl, ort))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The discrepencies are significant.\nThe ONNX model keeps float at every step.\n\n.. blockdiag::\n\n   diagram {\n     x_float32 -> normalizer -> y_float32 -> dtree -> z_float32\n   }\n\nIn :epkg:`scikit-learn`:\n\n.. blockdiag::\n\n   diagram {\n     x_float32 -> normalizer -> y_double -> dtree -> z_double\n   }\n\n## CastTransformer\n\nWe could try to use double everywhere. Unfortunately,\n:epkg:`ONNX ML Operators` only allows float coefficients\nfor the operator *TreeEnsembleRegressor*. We may want\nto compromise by casting the output of the normalizer into\nfloat in the :epkg:`scikit-learn` pipeline.\n\n.. blockdiag::\n\n   diagram {\n     x_float32 -> normalizer -> y_double ->\n     cast -> y_float -> dtree -> z_float\n   }\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model2 = Pipeline([\n    ('scaler', StandardScaler()),\n    ('cast', CastTransformer()),\n    ('dt', DecisionTreeRegressor(max_depth=max_depth))\n])\n\nmodel2.fit(Xi_train, yi_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The discrepencies.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "onx2 = to_onnx(model2, Xi_train[:1].astype(numpy.float32))\n\nsess2 = InferenceSession(onx2.SerializeToString())\n\nskl2 = model2.predict(X32)\nort2 = sess2.run(None, {'X': X32})[0]\n\nprint(diff(skl2, ort2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That still fails because the normalizer\nin :epkg:`scikit-learn` and in :epkg:`ONNX`\nuse different types. The cast still happens and\nthe *dx* is still here. To remove it, we need to use\ndouble in ONNX normalizer.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model3 = Pipeline([\n    ('cast64', CastTransformer(dtype=numpy.float64)),\n    ('scaler', StandardScaler()),\n    ('cast', CastTransformer()),\n    ('dt', DecisionTreeRegressor(max_depth=max_depth))\n])\n\nmodel3.fit(Xi_train, yi_train)\nonx3 = to_onnx(model3, Xi_train[:1].astype(numpy.float32),\n               options={StandardScaler: {'div': 'div_cast'}})\n\nsess3 = InferenceSession(onx3.SerializeToString())\n\nskl3 = model3.predict(X32)\nort3 = sess3.run(None, {'X': X32})[0]\n\nprint(diff(skl3, ort3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It works. That also means that it is difficult to change\nthe computation type when a pipeline includes a discontinuous\nfunction. It is better to keep the same types all along\nbefore using a decision tree.\n\n## Sledgehammer\n\nThe idea here is to always train the next step based\non ONNX outputs. That way, every step of the pipeline\nis trained based on ONNX output.\n\n* Trains the first step.\n* Converts the step into ONNX\n* Computes ONNX outputs.\n* Trains the second step on these outputs.\n* Converts the second step into ONNX.\n* Merges it with the first step.\n* Computes ONNX outputs of the merged two first steps.\n* ...\n\nIt is implemented in\nclass :epkg:`OnnxPipeline`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_onx = OnnxPipeline([\n    ('scaler', StandardScaler()),\n    ('dt', DecisionTreeRegressor(max_depth=max_depth))\n])\n\nmodel_onx.fit(Xi_train, yi_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The conversion.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "onx4 = to_onnx(model_onx, Xi_train[:1].astype(numpy.float32))\n\nsess4 = InferenceSession(onx4.SerializeToString())\n\nskl4 = model_onx.predict(X32)\nort4 = sess4.run(None, {'X': X32})[0]\n\nprint(diff(skl4, ort4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It works too in a more simple way.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## No discrepencies at all?\n\nIs it possible to get no error at all?\nThere is one major obstacle: :epkg:`scikit-learn`\nstores the predicted values in every leave with double\n(`_tree.pyx - _get_value_ndarray\n<https://github.com/scikit-learn/scikit-learn/blob/master/\nsklearn/tree/_tree.pyx#L1096>`_), :epkg:`ONNX` defines the\nthe predicted values as floats: :epkg:`TreeEnsembleRegressor`.\nWhat can we do to solve it?\nWhat if we could extend ONNX specifications to support\ndouble instead of floats.\nWe reuse what was developped in example\n`Other way to convert <http://www.xavierdupre.fr/app/\nmlprodict/helpsphinx/notebooks/onnx_discrepencies.html\n?highlight=treeensembleregressordouble#other-way-to-convert>`_\nand a custom ONNX node `TreeEnsembleRegressorDouble\n<http://www.xavierdupre.fr/app/mlprodict/helpsphinx/api/onnxrt_ops.html\n?highlight=treeensembleregressordouble#treeensembleregressordouble>`_.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tree = DecisionTreeRegressor(max_depth=max_depth)\ntree.fit(Xi_train, yi_train)\n\nmodel_onx = to_onnx_extended(tree, Xi_train[:1].astype(numpy.float64),\n                             rewrite_ops=True)\n\noinf5 = OnnxInference(model_onx, runtime='python_compiled')\nprint(oinf5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's measure the discrepencies.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X64 = Xi_test.astype(numpy.float64)\nskl5 = tree.predict(X64)\nort5 = oinf5.run({'X': X64})['variable']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Perfect, no discrepencies at all.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(diff(skl5, ort5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CastRegressor\n\nThe previous example demonstrated the type difference for\nthe predicted values explains the small differences between\n:epkg:`scikit-learn` and :epkg:`onnxruntime`. But it does not\nwith the current ONNX. Another option is to cast the\nthe predictions into floats in the :epkg:`scikit-learn` pipeline.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ctree = CastRegressor(DecisionTreeRegressor(max_depth=max_depth))\nctree.fit(Xi_train, yi_train)\n\nonx6 = to_onnx(ctree, Xi_train[:1].astype(numpy.float32))\n\nsess6 = InferenceSession(onx6.SerializeToString())\n\nskl6 = ctree.predict(X32)\nort6 = sess6.run(None, {'X': X32})[0]\n\nprint(diff(skl6, ort6))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Success!\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}