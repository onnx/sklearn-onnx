{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# TfIdfVectorizer with ONNX\n\nThis example is inspired from the following example:\n`Column Transformer with Heterogeneous Data Sources\n<https://scikit-learn.org/stable/auto_examples/\ncompose/plot_column_transformer.html>`_\nwhich builds a pipeline to classify text.\n\n## Train a pipeline with TfidfVectorizer\n\nIt replicates the same pipeline taken from *scikit-learn* documentation\nbut reduces it to the part ONNX actually supports without implementing\na custom converter. Let's get the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport os\nfrom onnx.tools.net_drawer import GetPydotGraph, GetOpNodeProducer\nimport numpy\nimport onnxruntime as rt\nfrom skl2onnx.common.data_types import StringTensorType\nfrom skl2onnx import convert_sklearn\nimport numpy as np\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.datasets import fetch_20newsgroups\ntry:\n    from sklearn.datasets._twenty_newsgroups import (\n        strip_newsgroup_footer, strip_newsgroup_quoting)\nexcept ImportError:\n    # scikit-learn < 0.24\n    from sklearn.datasets.twenty_newsgroups import (\n        strip_newsgroup_footer, strip_newsgroup_quoting)\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import classification_report\nfrom sklearn.linear_model import LogisticRegression\n\n\n# limit the list of categories to make running this example faster.\ncategories = ['alt.atheism', 'talk.religion.misc']\ntrain = fetch_20newsgroups(random_state=1,\n                           subset='train',\n                           categories=categories,\n                           )\ntest = fetch_20newsgroups(random_state=1,\n                          subset='test',\n                          categories=categories,\n                          )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first transform extract two fields from the data.\nWe take it out form the pipeline and assume\nthe data is defined by two text columns.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class SubjectBodyExtractor(BaseEstimator, TransformerMixin):\n    \"\"\"Extract the subject & body from a usenet post in a single pass.\n    Takes a sequence of strings and produces a dict of sequences. Keys are\n    `subject` and `body`.\n    \"\"\"\n\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, posts):\n        # construct object dtype array with two columns\n        # first column = 'subject' and second column = 'body'\n        features = np.empty(shape=(len(posts), 2), dtype=object)\n        for i, text in enumerate(posts):\n            headers, _, bod = text.partition('\\n\\n')\n            bod = strip_newsgroup_footer(bod)\n            bod = strip_newsgroup_quoting(bod)\n            features[i, 1] = bod\n\n            prefix = 'Subject:'\n            sub = ''\n            for line in headers.split('\\n'):\n                if line.startswith(prefix):\n                    sub = line[len(prefix):]\n                    break\n            features[i, 0] = sub\n\n        return features\n\n\ntrain_data = SubjectBodyExtractor().fit_transform(train.data)\ntest_data = SubjectBodyExtractor().fit_transform(test.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The pipeline is almost the same except\nwe remove the custom features.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipeline = Pipeline([\n    ('union', ColumnTransformer(\n        [\n            ('subject', TfidfVectorizer(min_df=50), 0),\n\n            ('body_bow', Pipeline([\n                ('tfidf', TfidfVectorizer()),\n                ('best', TruncatedSVD(n_components=50)),\n            ]), 1),\n\n            # Removed from the original example as\n            # it requires a custom converter.\n            # ('body_stats', Pipeline([\n            #   ('stats', TextStats()),  # returns a list of dicts\n            #   ('vect', DictVectorizer()),  # list of dicts -> feature matrix\n            # ]), 1),\n        ],\n\n        transformer_weights={\n            'subject': 0.8,\n            'body_bow': 0.5,\n            # 'body_stats': 1.0,\n        }\n    )),\n\n    # Use a LogisticRegression classifier on the combined features.\n    # Instead of LinearSVC (not fully ready in onnxruntime).\n    ('logreg', LogisticRegression()),\n])\n\npipeline.fit(train_data, train.target)\nprint(classification_report(pipeline.predict(test_data), test.target))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ONNX conversion\n\nIt is difficult to replicate the exact same tokenizer\nbehaviour if the tokeniser comes from space, gensim or nltk.\nThe default one used by *scikit-learn* uses regular expressions\nand is currently being implementing. The current implementation\nonly considers a list of separators which can is defined\nin variable *seps*.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "seps = {\n    TfidfVectorizer: {\n        \"separators\": [\n            ' ', '.', '\\\\?', ',', ';', ':', '!',\n            '\\\\(', '\\\\)', '\\n', '\"', \"'\",\n            \"-\", \"\\\\[\", \"\\\\]\", \"@\"\n        ]\n    }\n}\nmodel_onnx = convert_sklearn(\n    pipeline, \"tfidf\",\n    initial_types=[(\"input\", StringTensorType([None, 2]))],\n    options=seps, target_opset=12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And save.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with open(\"pipeline_tfidf.onnx\", \"wb\") as f:\n    f.write(model_onnx.SerializeToString())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Predictions with onnxruntime.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sess = rt.InferenceSession(\"pipeline_tfidf.onnx\")\nprint('---', train_data[0])\ninputs = {'input': train_data[:1]}\npred_onx = sess.run(None, inputs)\nprint(\"predict\", pred_onx[0])\nprint(\"predict_proba\", pred_onx[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With *scikit-learn*:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(pipeline.predict(train_data[:1]))\nprint(pipeline.predict_proba(train_data[:1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are discrepencies for this model because\nthe tokenization is not exactly the same.\nThis is a work in progress.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Display the ONNX graph\n\nFinally, let's see the graph converted with *sklearn-onnx*.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pydot_graph = GetPydotGraph(\n    model_onnx.graph, name=model_onnx.graph.name,\n    rankdir=\"TB\", node_producer=GetOpNodeProducer(\"docstring\",\n                                                  color=\"yellow\",\n                                                  fillcolor=\"yellow\",\n                                                  style=\"filled\"))\npydot_graph.write_dot(\"pipeline_tfidf.dot\")\n\nos.system('dot -O -Gdpi=300 -Tpng pipeline_tfidf.dot')\n\nimage = plt.imread(\"pipeline_tfidf.dot.png\")\nfig, ax = plt.subplots(figsize=(40, 20))\nax.imshow(image)\nax.axis('off')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}