{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# TfIdf and sparse matrices\n\n.. index:: xgboost, lightgbm, sparse, ensemble\n\n`TfidfVectorizer <https://scikit-learn.org/stable/modules/\ngenerated/sklearn.feature_extraction.text.TfidfVectorizer.html>`_\nusually creates sparse data. If the data is sparse enough, matrices\nusually stays as sparse all along the pipeline until the predictor\nis trained. Sparse matrices do not consider null and missing values\nas they are not present in the datasets. Because some predictors\ndo the difference, this ambiguity may introduces discrepencies\nwhen converter into ONNX. This example looks into several configurations.\n\n## Imports, setups\n\nAll imports. It also registered onnx converters for :epgk:`xgboost`\nand *lightgbm*.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import warnings\nimport numpy\nimport pandas\nimport onnxruntime as rt\nfrom tqdm import tqdm\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.datasets import load_iris\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.ensemble import RandomForestClassifier\ntry:\n    from sklearn.ensemble import HistGradientBoostingClassifier\nexcept ImportError:\n    HistGradientBoostingClassifier = None\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom skl2onnx.common.data_types import FloatTensorType, StringTensorType\nfrom skl2onnx import to_onnx, update_registered_converter\nfrom skl2onnx.sklapi import CastTransformer, ReplaceTransformer\nfrom skl2onnx.common.shape_calculator import (\n    calculate_linear_classifier_output_shapes)\nfrom onnxmltools.convert.xgboost.operator_converters.XGBoost import (\n    convert_xgboost)\nfrom onnxmltools.convert.lightgbm.operator_converters.LightGbm import (\n    convert_lightgbm)\n\n\nupdate_registered_converter(\n    XGBClassifier, 'XGBoostXGBClassifier',\n    calculate_linear_classifier_output_shapes, convert_xgboost,\n    options={'nocl': [True, False], 'zipmap': [True, False, 'columns']})\nupdate_registered_converter(\n    LGBMClassifier, 'LightGbmLGBMClassifier',\n    calculate_linear_classifier_output_shapes, convert_lightgbm,\n    options={'nocl': [True, False], 'zipmap': [True, False]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Artificial datasets\n\nIris + a text column.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cst = ['class zero', 'class one', 'class two']\n\ndata = load_iris()\nX = data.data[:, :2]\ny = data.target\n\ndf = pandas.DataFrame(X)\ndf[\"text\"] = [cst[i] for i in y]\n\n\nind = numpy.arange(X.shape[0])\nnumpy.random.shuffle(ind)\nX = X[ind, :].copy()\ny = y[ind].copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train ensemble after sparse\n\nThe example use the Iris datasets with artifical text datasets\npreprocessed with a tf-idf. `sparse_threshold=1.` avoids\nsparse matrices to be converted into dense matrices.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def make_pipelines(df_train, y_train, models=None,\n                   sparse_threshold=1., replace_nan=False,\n                   insert_replace=False):\n\n    if models is None:\n        models = [\n            RandomForestClassifier, HistGradientBoostingClassifier,\n            XGBClassifier, LGBMClassifier]\n    models = [_ for _ in models if _ is not None]\n\n    pipes = []\n    for model in tqdm(models):\n\n        if model == HistGradientBoostingClassifier:\n            kwargs = dict(max_iter=5)\n        elif model == XGBClassifier:\n            kwargs = dict(n_estimators=5, use_label_encoder=False)\n        else:\n            kwargs = dict(n_estimators=5)\n\n        if insert_replace:\n            pipe = Pipeline([\n                ('union', ColumnTransformer([\n                    ('scale1', StandardScaler(), [0, 1]),\n                    ('subject',\n                     Pipeline([\n                         ('count', CountVectorizer()),\n                         ('tfidf', TfidfTransformer()),\n                         ('repl', ReplaceTransformer()),\n                     ]), \"text\"),\n                ], sparse_threshold=sparse_threshold)),\n                ('cast', CastTransformer()),\n                ('cls', model(max_depth=3, **kwargs)),\n            ])\n        else:\n            pipe = Pipeline([\n                ('union', ColumnTransformer([\n                    ('scale1', StandardScaler(), [0, 1]),\n                    ('subject',\n                     Pipeline([\n                         ('count', CountVectorizer()),\n                         ('tfidf', TfidfTransformer())\n                     ]), \"text\"),\n                ], sparse_threshold=sparse_threshold)),\n                ('cast', CastTransformer()),\n                ('cls', model(max_depth=3, **kwargs)),\n            ])\n\n        try:\n            pipe.fit(df_train, y_train)\n        except TypeError as e:\n            obs = dict(model=model.__name__, pipe=pipe, error=e)\n            pipes.append(obs)\n            continue\n\n        options = {model: {'zipmap': False}}\n        if replace_nan:\n            options[TfidfTransformer] = {'nan': True}\n\n        # convert\n        with warnings.catch_warnings(record=False):\n            warnings.simplefilter(\"ignore\", (FutureWarning, UserWarning))\n            model_onnx = to_onnx(\n                pipe,\n                initial_types=[('input', FloatTensorType([None, 2])),\n                               ('text', StringTensorType([None, 1]))],\n                target_opset=12, options=options)\n\n        with open('model.onnx', 'wb') as f:\n            f.write(model_onnx.SerializeToString())\n\n        sess = rt.InferenceSession(model_onnx.SerializeToString())\n        inputs = {\"input\": df[[0, 1]].values.astype(numpy.float32),\n                  \"text\": df[[\"text\"]].values}\n        pred_onx = sess.run(None, inputs)\n\n        diff = numpy.abs(\n            pred_onx[1].ravel() -\n            pipe.predict_proba(df).ravel()).sum()\n\n        obs = dict(model=model.__name__,\n                   discrepencies=diff,\n                   model_onnx=model_onnx, pipe=pipe)\n        pipes.append(obs)\n\n    return pipes\n\n\ndata_sparse = make_pipelines(df, y)\nstat = pandas.DataFrame(data_sparse).drop(['model_onnx', 'pipe'], axis=1)\nif 'error' in stat.columns:\n    print(stat.drop('error', axis=1))\nstat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sparse data hurts.\n\n## Dense data\n\nLet's replace sparse data with dense by using `sparse_threshold=0.`\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_dense = make_pipelines(df, y, sparse_threshold=0.)\nstat = pandas.DataFrame(data_dense).drop(['model_onnx', 'pipe'], axis=1)\nif 'error' in stat.columns:\n    print(stat.drop('error', axis=1))\nstat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is much better. Let's compare how the preprocessing\napplies on the data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"sparse\")\nprint(data_sparse[-1]['pipe'].steps[0][-1].transform(df)[:2])\nprint()\nprint(\"dense\")\nprint(data_dense[-1]['pipe'].steps[0][-1].transform(df)[:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This shows `RandomForestClassifier\n<https://scikit-learn.org/stable/modules/generated/\nsklearn.ensemble.RandomForestClassifier.html>`_,\n`XGBClassifier <https://xgboost.readthedocs.io/\nen/latest/python/python_api.html>`_ do not process\nthe same way sparse and\ndense matrix as opposed to `LGBMClassifier\n<https://lightgbm.readthedocs.io/en/latest/\npythonapi/lightgbm.LGBMClassifier.html>`_.\nAnd `HistGradientBoostingClassifier\n<https://scikit-learn.org/stable/modules/generated/\nsklearn.ensemble.HistGradientBoostingClassifier.html>`_\nfails.\n\n## Dense data with nan\n\nLet's keep sparse data in the scikit-learn pipeline but\nreplace null values by nan in the onnx graph.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_dense = make_pipelines(df, y, sparse_threshold=1., replace_nan=True)\nstat = pandas.DataFrame(data_dense).drop(['model_onnx', 'pipe'], axis=1)\nif 'error' in stat.columns:\n    print(stat.drop('error', axis=1))\nstat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dense, 0 replaced by nan\n\nInstead of using a specific options to replace null values\ninto nan values, a custom transformer called\nReplaceTransformer is explicitely inserted into the pipeline.\nA new converter is added to the list of supported models.\nIt is equivalent to the previous options except it is\nmore explicit.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_dense = make_pipelines(df, y, sparse_threshold=1., replace_nan=False,\n                            insert_replace=True)\nstat = pandas.DataFrame(data_dense).drop(['model_onnx', 'pipe'], axis=1)\nif 'error' in stat.columns:\n    print(stat.drop('error', axis=1))\nstat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nUnless dense arrays are used, because *onnxruntime*\nONNX does not support sparse yet, the conversion needs to be\ntuned depending on the model which follows the TfIdf preprocessing.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}