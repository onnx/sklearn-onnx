{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Issues when switching to float\n\n.. index:: float, double, discrepencies\n\nMost models in :epkg:`scikit-learn` do computation with double,\nnot float. Most models in deep learning use float because\nthat's the most common situation with GPU. ONNX was initially\ncreated to facilitate the deployment of deep learning models\nand that explains why many converters assume the converted models\nshould use float. That assumption does not usually harm\nthe predictions, the conversion to float introduce small\ndiscrepencies compare to double predictions.\nThat assumption is usually true if the prediction\nfunction is continuous, $y = f(x)$, then\n$dy = f'(x) dx$. We can determine an upper bound\nto the discrepencies :\n$\\Delta(y) \\leqslant \\sup_x \\left\\Vert f'(x)\\right\\Vert dx$.\n*dx* is the discrepency introduced by a float conversion,\n``dx = x - numpy.float32(x)``.\n\nHowever, that's not the case for every model. A decision tree\ntrained for a regression is not a continuous function. Therefore,\neven a small *dx* may introduce a huge discrepency. Let's look into\nan example which always produces discrepencies and some ways\nto overcome this situation.\n\n## More into the issue\n\nThe below example is built to fail.\nIt contains integer features with different order\nof magnitude rounded to integer. A decision tree compares\nfeatures to thresholds. In most cases, float and double\ncomparison gives the same result. We denote\n$[x]_{f32}$ the conversion (or cast)\n``numpy.float32(x)``.\n\n\\begin{align}x \\leqslant y = [x]_{f32} \\leqslant [y]_{f32}\\end{align}\n\nHowever, the probability that both comparisons give\ndifferent results is not null. The following graph shows\nthe discord areas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skl2onnx.sklapi import CastTransformer\nfrom skl2onnx import to_onnx\nfrom onnxruntime import InferenceSession\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.datasets import make_regression\nimport numpy\nimport matplotlib.pyplot as plt\n\n\ndef area_mismatch_rule(N, delta, factor, rule=None):\n    if rule is None:\n\n        def rule(t):\n            return numpy.float32(t)\n\n    xst = []\n    yst = []\n    xsf = []\n    ysf = []\n    for x in range(-N, N):\n        for y in range(-N, N):\n            dx = (1.0 + x * delta) * factor\n            dy = (1.0 + y * delta) * factor\n            c1 = 1 if numpy.float64(dx) <= numpy.float64(dy) else 0\n            c2 = 1 if numpy.float32(dx) <= rule(dy) else 0\n            key = abs(c1 - c2)\n            if key == 1:\n                xsf.append(dx)\n                ysf.append(dy)\n            else:\n                xst.append(dx)\n                yst.append(dy)\n    return xst, yst, xsf, ysf\n\n\ndelta = 36e-10\nfactor = 1\nxst, yst, xsf, ysf = area_mismatch_rule(100, delta, factor)\n\n\nfig, ax = plt.subplots(1, 1, figsize=(5, 5))\nax.plot(xst, yst, \".\", label=\"agree\")\nax.plot(xsf, ysf, \".\", label=\"disagree\")\nax.set_title(\"Region where x <= y and (float)x <= (float)y agree\")\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y\")\nax.plot([min(xst), max(xst)], [min(yst), max(yst)], \"k--\")\nax.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The pipeline and the data\n\nWe can now build an example where the learned decision tree\ndoes many comparisons in this discord area. This is done\nby rounding features to integers, a frequent case\nhappening when dealing with categorical features.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y = make_regression(10000, 10)\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\nXi_train, yi_train = X_train.copy(), y_train.copy()\nXi_test, yi_test = X_test.copy(), y_test.copy()\nfor i in range(X.shape[1]):\n    Xi_train[:, i] = (Xi_train[:, i] * 2**i).astype(numpy.int64)\n    Xi_test[:, i] = (Xi_test[:, i] * 2**i).astype(numpy.int64)\n\nmax_depth = 10\n\nmodel = Pipeline(\n    [(\"scaler\", StandardScaler()), (\"dt\", DecisionTreeRegressor(max_depth=max_depth))]\n)\n\nmodel.fit(Xi_train, yi_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The discrepencies\n\nLet's reuse the function implemented in the\nfirst example `l-diff-dicrepencies` and\nlook into the conversion.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def diff(p1, p2):\n    p1 = p1.ravel()\n    p2 = p2.ravel()\n    d = numpy.abs(p2 - p1)\n    return d.max(), (d / numpy.abs(p1)).max()\n\n\nonx = to_onnx(model, Xi_train[:1].astype(numpy.float32), target_opset=15)\n\nsess = InferenceSession(onx.SerializeToString(), providers=[\"CPUExecutionProvider\"])\n\nX32 = Xi_test.astype(numpy.float32)\n\nskl = model.predict(X32)\nort = sess.run(None, {\"X\": X32})[0]\n\nprint(diff(skl, ort))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The discrepencies are significant.\nThe ONNX model keeps float at every step.\n\n.. blockdiag::\n\n   diagram {\n     x_float32 -> normalizer -> y_float32 -> dtree -> z_float32\n   }\n\nIn :epkg:`scikit-learn`:\n\n.. blockdiag::\n\n   diagram {\n     x_float32 -> normalizer -> y_double -> dtree -> z_double\n   }\n\n## CastTransformer\n\nWe could try to use double everywhere. Unfortunately,\n:epkg:`ONNX ML Operators` only allows float coefficients\nfor the operator *TreeEnsembleRegressor*. We may want\nto compromise by casting the output of the normalizer into\nfloat in the :epkg:`scikit-learn` pipeline.\n\n.. blockdiag::\n\n   diagram {\n     x_float32 -> normalizer -> y_double ->\n     cast -> y_float -> dtree -> z_float\n   }\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model2 = Pipeline(\n    [\n        (\"scaler\", StandardScaler()),\n        (\"cast\", CastTransformer()),\n        (\"dt\", DecisionTreeRegressor(max_depth=max_depth)),\n    ]\n)\n\nmodel2.fit(Xi_train, yi_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The discrepencies.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "onx2 = to_onnx(model2, Xi_train[:1].astype(numpy.float32), target_opset=15)\n\nsess2 = InferenceSession(onx2.SerializeToString(), providers=[\"CPUExecutionProvider\"])\n\nskl2 = model2.predict(X32)\nort2 = sess2.run(None, {\"X\": X32})[0]\n\nprint(diff(skl2, ort2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That still fails because the normalizer\nin :epkg:`scikit-learn` and in :epkg:`ONNX`\nuse different types. The cast still happens and\nthe *dx* is still here. To remove it, we need to use\ndouble in ONNX normalizer.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model3 = Pipeline(\n    [\n        (\"cast64\", CastTransformer(dtype=numpy.float64)),\n        (\"scaler\", StandardScaler()),\n        (\"cast\", CastTransformer()),\n        (\"dt\", DecisionTreeRegressor(max_depth=max_depth)),\n    ]\n)\n\nmodel3.fit(Xi_train, yi_train)\nonx3 = to_onnx(\n    model3,\n    Xi_train[:1].astype(numpy.float32),\n    options={StandardScaler: {\"div\": \"div_cast\"}},\n    target_opset=15,\n)\n\nsess3 = InferenceSession(onx3.SerializeToString(), providers=[\"CPUExecutionProvider\"])\n\nskl3 = model3.predict(X32)\nort3 = sess3.run(None, {\"X\": X32})[0]\n\nprint(diff(skl3, ort3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It works. That also means that it is difficult to change\nthe computation type when a pipeline includes a discontinuous\nfunction. It is better to keep the same types all along\nbefore using a decision tree.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}