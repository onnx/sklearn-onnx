{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# FeatureHasher, pandas values and unexpected discrepancies\n\nA game of finding it goes wrong and there are multiple places.\n\n\n## Initial example\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import logging\nimport numpy as np\nfrom pandas import DataFrame\nfrom onnxruntime import InferenceSession, SessionOptions\nfrom onnxruntime_extensions import get_library_path\nfrom sklearn.feature_extraction import FeatureHasher\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom skl2onnx import to_onnx\nfrom skl2onnx.common.data_types import StringTensorType\n\nlog = logging.getLogger(\"skl2onnx\")\nlog.setLevel(logging.ERROR)\n\n\ndf = DataFrame(\n    {\n        \"Cat1\": [\"a\", \"b\", \"d\", \"abd\", \"e\", \"z\", \"ez\"],\n        \"Cat2\": [\"A\", \"B\", \"D\", \"ABD\", \"e\", \"z\", \"ez\"],\n        \"Label\": [1, 1, 0, 0, 1, 0, 0],\n    }\n)\n\ncat_features = [c for c in df.columns if \"Cat\" in c]\nX_train = df[cat_features]\n\nX_train[\"cat_features\"] = df[cat_features].values.tolist()\nX_train = X_train.drop(cat_features, axis=1)\ny_train = df[\"Label\"]\n\npipe = Pipeline(\n    steps=[\n        (\n            \"preprocessor\",\n            ColumnTransformer(\n                [\n                    (\n                        \"cat_preprocessor\",\n                        FeatureHasher(\n                            n_features=8,\n                            input_type=\"string\",\n                            alternate_sign=False,\n                            dtype=np.float32,\n                        ),\n                        \"cat_features\",\n                    )\n                ],\n                sparse_threshold=0.0,\n            ),\n        ),\n        (\"classifier\", GradientBoostingClassifier(n_estimators=2, max_depth=2)),\n    ],\n)\npipe.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conversion to ONNX.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "onx = to_onnx(\n    pipe,\n    initial_types=[(\"cat_features\", StringTensorType([None, None]))],\n    options={\"zipmap\": False},\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are many discrepancies?\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "expected_proba = pipe.predict_proba(X_train)\nsess = InferenceSession(onx.SerializeToString(), providers=[\"CPUExecutionProvider\"])\n\n\ngot = sess.run(None, dict(cat_features=X_train.values))\n\n\nprint(\"expected probabilities\")\nprint(expected_proba)\n\nprint(\"onnx probabilities\")\nprint(got[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Let's check the feature hasher\n\nWe just remove the classifier.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipe_hash = Pipeline(\n    steps=[\n        (\n            \"preprocessor\",\n            ColumnTransformer(\n                [\n                    (\n                        \"cat_preprocessor\",\n                        FeatureHasher(\n                            n_features=8,\n                            input_type=\"string\",\n                            alternate_sign=False,\n                            dtype=np.float32,\n                        ),\n                        \"cat_features\",\n                    )\n                ],\n                sparse_threshold=0.0,\n            ),\n        ),\n    ],\n)\npipe_hash.fit(X_train, y_train)\n\nonx = to_onnx(\n    pipe_hash,\n    initial_types=[(\"cat_features\", StringTensorType([None, None]))],\n    options={\"zipmap\": False},\n)\n\nexpected = pipe_hash.transform(X_train)\nsess = InferenceSession(onx.SerializeToString(), providers=[\"CPUExecutionProvider\"])\n\n\ngot = sess.run(None, dict(cat_features=X_train.values))\n\n\nprint(\"expected hashed features\")\nprint(expected)\n\nprint(\"onnx hashed features\")\nprint(got[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nothing seems to be working.\n\n## First proposal\n\nThe instruction\n``X_train[\"cat_features\"] = df[cat_features].values.tolist()``\ncreates a DataFrame with on column of a lists of two values.\nThe type list is expected by scikit-learn and it can process a variable\nnumber of elements per list. onnxruntime cannot do that.\nIt must be changed into the following.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipe_hash = Pipeline(\n    steps=[\n        (\n            \"preprocessor\",\n            ColumnTransformer(\n                [\n                    (\n                        \"cat_preprocessor1\",\n                        FeatureHasher(\n                            n_features=8,\n                            input_type=\"string\",\n                            alternate_sign=False,\n                            dtype=np.float32,\n                        ),\n                        [0],\n                    ),\n                    (\n                        \"cat_preprocessor2\",\n                        FeatureHasher(\n                            n_features=8,\n                            input_type=\"string\",\n                            alternate_sign=False,\n                            dtype=np.float32,\n                        ),\n                        [1],\n                    ),\n                ],\n                sparse_threshold=0.0,\n            ),\n        ),\n    ],\n)\n\nX_train_skl = df[cat_features].copy()\nfor c in cat_features:\n    X_train_skl[c] = X_train_skl[c].values.tolist()\n\npipe_hash.fit(X_train_skl.values, y_train)\n\nonx = to_onnx(\n    pipe_hash,\n    initial_types=[\n        (\"cat1\", StringTensorType([None, 1])),\n        (\"cat2\", StringTensorType([None, 1])),\n    ],\n    options={\"zipmap\": False},\n)\n\n\nexpected = pipe_hash.transform(X_train_skl.values)\nsess = InferenceSession(onx.SerializeToString(), providers=[\"CPUExecutionProvider\"])\n\n\ngot = sess.run(\n    None,\n    dict(\n        cat1=df[\"Cat1\"].values.reshape((-1, 1)), cat2=df[\"Cat2\"].values.reshape((-1, 1))\n    ),\n)\n\n\nprint(\"expected fixed hashed features\")\nprint(expected)\n\nprint(\"onnx fixed hashed features\")\nprint(got[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is not the original pipeline. It has 16 columns instead of 8\nbut it does produce the same results.\nOne option would be to add the first 8 columns to the other 8\nby using a custom converter.\n\n## Second proposal\n\nWe use the same initial pipeline but we tweak the input\nonnxruntime receives.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipe_hash = Pipeline(\n    steps=[\n        (\n            \"preprocessor\",\n            ColumnTransformer(\n                [\n                    (\n                        \"cat_preprocessor\",\n                        FeatureHasher(\n                            n_features=8,\n                            input_type=\"string\",\n                            alternate_sign=False,\n                            dtype=np.float32,\n                        ),\n                        \"cat_features\",\n                    )\n                ],\n                sparse_threshold=0.0,\n            ),\n        ),\n    ],\n)\npipe_hash.fit(X_train, y_train)\n\nonx = to_onnx(\n    pipe_hash,\n    initial_types=[(\"cat_features\", StringTensorType([None, 1]))],\n    options={\"zipmap\": False, \"preprocessor__cat_preprocessor__separator\": \"#\"},\n)\n\nexpected = pipe_hash.transform(X_train)\n\n\nso = SessionOptions()\nso.register_custom_ops_library(get_library_path())\nsess = InferenceSession(onx.SerializeToString(), so, providers=[\"CPUExecutionProvider\"])\n\n# We merged both columns cat1 and cat2 into a single cat_features.\ndf_fixed = DataFrame()\ndf_fixed[\"cat_features\"] = np.array([f\"{a}#{b}\" for a, b in X_train[\"cat_features\"]])\n\ngot = sess.run(None, {\"cat_features\": df_fixed[[\"cat_features\"]].values})\n\nprint(\"expected original hashed features\")\nprint(expected)\n\nprint(\"onnx fixed original hashed features\")\nprint(got[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It works now.\n\n## Sparsity?\n\nLet's try with the classifier now and no `sparse_threshold=0.0`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipe = Pipeline(\n    steps=[\n        (\n            \"preprocessor\",\n            ColumnTransformer(\n                [\n                    (\n                        \"cat_preprocessor\",\n                        FeatureHasher(\n                            n_features=8,\n                            input_type=\"string\",\n                            alternate_sign=False,\n                            dtype=np.float32,\n                        ),\n                        \"cat_features\",\n                    )\n                ],\n                # sparse_threshold=0.0,\n            ),\n        ),\n        (\"classifier\", GradientBoostingClassifier(n_estimators=2, max_depth=2)),\n    ],\n)\npipe.fit(X_train, y_train)\nexpected = pipe.predict_proba(X_train)\n\n\nonx = to_onnx(\n    pipe,\n    initial_types=[(\"cat_features\", StringTensorType([None, 1]))],\n    options={\"zipmap\": False, \"preprocessor__cat_preprocessor__separator\": \"#\"},\n)\n\nso = SessionOptions()\nso.register_custom_ops_library(get_library_path())\nsess = InferenceSession(onx.SerializeToString(), so, providers=[\"CPUExecutionProvider\"])\ngot = sess.run(None, {\"cat_features\": df_fixed[[\"cat_features\"]].values})\n\n\nprint(\"expected probabilies\")\nprint(expected)\n\nprint(\"onnx probabilies\")\nprint(got[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "scikit-learn keeps the sparse outputs from\nthe FeatureHasher. onnxruntime does not support\nsparse features. This may have an impact on the conversion\nif the model next to this step makes a difference between a\nmissing sparse value and zero.\nThat does not seem to be the case for this model but\nother models or libraries may behave differently.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(pipe.steps[0][-1].transform(X_train))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}