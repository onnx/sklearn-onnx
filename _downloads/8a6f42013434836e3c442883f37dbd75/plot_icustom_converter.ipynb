{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Implement a new converter\n\n.. index:: custom converter\n\nBy default, :epkg:`sklearn-onnx` assumes that a classifier\nhas two outputs (label and probabilities), a regressor\nhas one output (prediction), a transform has one output\n(the transformed data). This example assumes the model to\nconvert is one of them. In that case, a new converter requires\nin fact two functions:\n\n* a shape calculator: it defines the output shape and type\n  based on the model and input type,\n* a converter: it actually builds an ONNX graph equivalent\n  to the prediction function to be converted.\n\nThis example implements both components for a new model.\n\n## Custom model\n\nLet's implement a simple custom model using\n:epkg:`scikit-learn` API. The model is preprocessing\nwhich decorrelates correlated random variables.\nIf *X* is a matrix of features, $V=\\frac{1}{n}X'X$\nis the covariance matrix. We compute $X V^{1/2}$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mlprodict.onnxrt import OnnxInference\nfrom pyquickhelper.helpgen.graphviz_helper import plot_graphviz\nimport pickle\nfrom io import BytesIO\nimport numpy\nfrom numpy.testing import assert_almost_equal\nfrom onnxruntime import InferenceSession\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.datasets import load_iris\nfrom skl2onnx.common.data_types import guess_numpy_type\nfrom skl2onnx import to_onnx\nfrom skl2onnx import update_registered_converter\nfrom skl2onnx.algebra.onnx_ops import OnnxMatMul, OnnxSub\n\n\nclass DecorrelateTransformer(TransformerMixin, BaseEstimator):\n    \"\"\"\n    Decorrelates correlated gaussian features.\n\n    :param alpha: avoids non inversible matrices\n        by adding *alpha* identity matrix\n\n    *Attributes*\n\n    * `self.mean_`: average\n    * `self.coef_`: square root of the coveriance matrix\n    \"\"\"\n\n    def __init__(self, alpha=0.):\n        BaseEstimator.__init__(self)\n        TransformerMixin.__init__(self)\n        self.alpha = alpha\n\n    def fit(self, X, y=None, sample_weights=None):\n        if sample_weights is not None:\n            raise NotImplementedError(\n                \"sample_weights != None is not implemented.\")\n        self.mean_ = numpy.mean(X, axis=0, keepdims=True)\n        X = X - self.mean_\n        V = X.T @ X / X.shape[0]\n        if self.alpha != 0:\n            V += numpy.identity(V.shape[0]) * self.alpha\n        L, P = numpy.linalg.eig(V)\n        Linv = L ** (-0.5)\n        diag = numpy.diag(Linv)\n        root = P @ diag @ P.transpose()\n        self.coef_ = root\n        return self\n\n    def transform(self, X):\n        return (X - self.mean_) @ self.coef_\n\n\ndef test_decorrelate_transformer():\n    data = load_iris()\n    X = data.data\n\n    dec = DecorrelateTransformer()\n    dec.fit(X)\n    pred = dec.transform(X)\n    cov = pred.T @ pred\n    cov /= cov[0, 0]\n    assert_almost_equal(numpy.identity(4), cov)\n\n    dec = DecorrelateTransformer(alpha=1e-10)\n    dec.fit(X)\n    pred = dec.transform(X)\n    cov = pred.T @ pred\n    cov /= cov[0, 0]\n    assert_almost_equal(numpy.identity(4), cov)\n\n    st = BytesIO()\n    pickle.dump(dec, st)\n    dec2 = pickle.load(BytesIO(st.getvalue()))\n    assert_almost_equal(dec.mean_, dec2.mean_)\n    assert_almost_equal(dec.coef_, dec2.coef_)\n    assert id(dec.mean_) != id(dec2.mean_)\n    assert id(dec.coef_) != id(dec2.coef_)\n\n\ntest_decorrelate_transformer()\n\ndata = load_iris()\nX = data.data\n\ndec = DecorrelateTransformer()\ndec.fit(X)\npred = dec.transform(X[:5])\nprint(pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trained coefficients.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(dec.mean_)\nprint(dec.coef_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conversion into ONNX\n\nLet's try to convert it and see what happens.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    to_onnx(dec, X.astype(numpy.float32))\nexcept Exception as e:\n    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This error means there is no converter associated\nto *DecorrelateTransformer*. Let's implement it.\nIt requires the two following\nfunctions, a shape calculator and a converter\nwith the same signature as below.\nFirst the shape calculator. We retrieve the input type\nadd tells the output type has the same type,\nthe same number of rows and a specific number of columns.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def decorrelate_transformer_shape_calculator(operator):\n    op = operator.raw_operator\n    input_type = operator.inputs[0].type.__class__\n    # The shape may be unknown. *get_first_dimension*\n    # returns the appropriate value, None in most cases\n    # meaning the transformer can process any batch of observations.\n    input_dim = operator.inputs[0].get_first_dimension()\n    output_type = input_type([input_dim, op.coef_.shape[1]])\n    operator.outputs[0].type = output_type"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The converter. One thing we need to pay attention to\nis the target opset. This information is important\nto make sure that every node is defined following the\nspecifications of that opset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def decorrelate_transformer_converter(scope, operator, container):\n    op = operator.raw_operator\n    opv = container.target_opset\n    out = operator.outputs\n\n    # We retrieve the unique input.\n    X = operator.inputs[0]\n\n    # In most case, computation happen in floats.\n    # But it might be with double. ONNX is very strict\n    # about types, every constant should have the same\n    # type as the input.\n    dtype = guess_numpy_type(X.type)\n\n    # We tell in ONNX language how to compute the unique output.\n    # op_version=opv tells which opset is requested\n    Y = OnnxMatMul(\n        OnnxSub(X, op.mean_.astype(dtype), op_version=opv),\n        op.coef_.astype(dtype),\n        op_version=opv, output_names=out[:1])\n    Y.add_to(scope, container)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need to let *skl2onnx* know about the new converter.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "update_registered_converter(\n    DecorrelateTransformer, \"SklearnDecorrelateTransformer\",\n    decorrelate_transformer_shape_calculator,\n    decorrelate_transformer_converter)\n\n\nonx = to_onnx(dec, X.astype(numpy.float32))\n\nsess = InferenceSession(onx.SerializeToString())\n\nexp = dec.transform(X.astype(numpy.float32))\ngot = sess.run(None, {'X': X.astype(numpy.float32)})[0]\n\n\ndef diff(p1, p2):\n    p1 = p1.ravel()\n    p2 = p2.ravel()\n    d = numpy.abs(p2 - p1)\n    return d.max(), (d / numpy.abs(p1)).max()\n\n\nprint(diff(exp, got))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's check it works as well with double.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "onx = to_onnx(dec, X.astype(numpy.float64))\n\nsess = InferenceSession(onx.SerializeToString())\n\nexp = dec.transform(X.astype(numpy.float64))\ngot = sess.run(None, {'X': X.astype(numpy.float64)})[0]\nprint(diff(exp, got))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The differences are smaller with double as expected.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final graph\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "oinf = OnnxInference(onx)\nax = plot_graphviz(oinf.to_dot())\nax.get_xaxis().set_visible(False)\nax.get_yaxis().set_visible(False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}