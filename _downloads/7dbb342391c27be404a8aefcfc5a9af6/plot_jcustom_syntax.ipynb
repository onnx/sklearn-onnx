{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Two ways to implement a converter\n\n.. index:: syntax\n\nThere are two ways to write a converter. The first one\nis less verbose and easier to understand\n(see `k_means.py <https://github.com/onnx/sklearn-onnx/blob/\nmaster/skl2onnx/operator_converters/k_means.py>`_).\nThe other is very verbose (see `ada_boost.py <https://github.com/onnx/\nsklearn-onnx/blob/master/skl2onnx/operator_converters/ada_boost.py>`_\nfor an example).\n\nThe first way is used in `l-plot-custom-converter`.\nThis one demonstrates the second way which is usually the one\nused in other converter library. It is more verbose.\n\n\n## Custom model\n\nIt basically copies what is in example\n``l-plot-custom-converter`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skl2onnx.common.data_types import guess_proto_type\nfrom onnxconverter_common.onnx_ops import apply_sub\nfrom onnxruntime import InferenceSession\nfrom skl2onnx import update_registered_converter\nfrom skl2onnx import to_onnx\nimport numpy\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.datasets import load_iris\n\n\nclass DecorrelateTransformer(TransformerMixin, BaseEstimator):\n    \"\"\"\n    Decorrelates correlated gaussian features.\n\n    :param alpha: avoids non inversible matrices\n        by adding *alpha* identity matrix\n\n    *Attributes*\n\n    * `self.mean_`: average\n    * `self.coef_`: square root of the coveriance matrix\n    \"\"\"\n\n    def __init__(self, alpha=0.):\n        BaseEstimator.__init__(self)\n        TransformerMixin.__init__(self)\n        self.alpha = alpha\n\n    def fit(self, X, y=None, sample_weights=None):\n        if sample_weights is not None:\n            raise NotImplementedError(\n                \"sample_weights != None is not implemented.\")\n        self.mean_ = numpy.mean(X, axis=0, keepdims=True)\n        X = X - self.mean_\n        V = X.T @ X / X.shape[0]\n        if self.alpha != 0:\n            V += numpy.identity(V.shape[0]) * self.alpha\n        L, P = numpy.linalg.eig(V)\n        Linv = L ** (-0.5)\n        diag = numpy.diag(Linv)\n        root = P @ diag @ P.transpose()\n        self.coef_ = root\n        return self\n\n    def transform(self, X):\n        return (X - self.mean_) @ self.coef_\n\n\ndata = load_iris()\nX = data.data\n\ndec = DecorrelateTransformer()\ndec.fit(X)\npred = dec.transform(X[:5])\nprint(pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conversion into ONNX\n\nThe shape calculator does not change.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def decorrelate_transformer_shape_calculator(operator):\n    op = operator.raw_operator\n    input_type = operator.inputs[0].type.__class__\n    # The shape may be unknown. *get_first_dimension*\n    # returns the appropriate value, None in most cases\n    # meaning the transformer can process any batch of observations.\n    input_dim = operator.inputs[0].get_first_dimension()\n    output_type = input_type([input_dim, op.coef_.shape[1]])\n    operator.outputs[0].type = output_type"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The converter is different.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def decorrelate_transformer_converter(scope, operator, container):\n    op = operator.raw_operator\n    out = operator.outputs\n\n    # We retrieve the unique input.\n    X = operator.inputs[0]\n\n    # In most case, computation happen in floats.\n    # But it might be with double. ONNX is very strict\n    # about types, every constant should have the same\n    # type as the input.\n    proto_dtype = guess_proto_type(X.type)\n\n    mean_name = scope.get_unique_variable_name('mean')\n    container.add_initializer(mean_name, proto_dtype,\n                              op.mean_.shape, list(op.mean_.ravel()))\n\n    coef_name = scope.get_unique_variable_name('coef')\n    container.add_initializer(coef_name, proto_dtype,\n                              op.coef_.shape, list(op.coef_.ravel()))\n\n    op_name = scope.get_unique_operator_name('sub')\n    sub_name = scope.get_unique_variable_name('sub')\n    # This function is defined in package onnxconverter_common.\n    # Most common operators can be added to the graph with\n    # these functions. It handles the case when specifications\n    # changed accross opsets (a parameter becomes an input\n    # for example).\n    apply_sub(scope, [X.full_name, mean_name], sub_name, container,\n              operator_name=op_name)\n\n    op_name = scope.get_unique_operator_name('matmul')\n    container.add_node(\n        'MatMul', [sub_name, coef_name],\n        out[0].full_name, name=op_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need to let *skl2onnx* know about the new converter.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "update_registered_converter(\n    DecorrelateTransformer, \"SklearnDecorrelateTransformer\",\n    decorrelate_transformer_shape_calculator,\n    decorrelate_transformer_converter)\n\n\nonx = to_onnx(dec, X.astype(numpy.float32))\n\nsess = InferenceSession(onx.SerializeToString())\n\nexp = dec.transform(X.astype(numpy.float32))\ngot = sess.run(None, {'X': X.astype(numpy.float32)})[0]\n\n\ndef diff(p1, p2):\n    p1 = p1.ravel()\n    p2 = p2.ravel()\n    d = numpy.abs(p2 - p1)\n    return d.max(), (d / numpy.abs(p1)).max()\n\n\nprint(diff(exp, got))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's check it works as well with double.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "onx = to_onnx(dec, X.astype(numpy.float64))\n\nsess = InferenceSession(onx.SerializeToString())\n\nexp = dec.transform(X.astype(numpy.float64))\ngot = sess.run(None, {'X': X.astype(numpy.float64)})[0]\nprint(diff(exp, got))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The differences are smaller with double as expected.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}