{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Intermediate results and investigation\n\n.. index:: investigate, intermediate results\n\nThere are many reasons why a user wants more than using\nthe converted model into ONNX. Intermediate results may be\nneeded, the output of every node in the graph. The ONNX\nmay need to be altered to remove some nodes.\nTransfer learning is usually removing the last layers of\na deep neural network. Another reaason is debugging.\nIt often happens that the runtime fails to compute the predictions\ndue to a shape mismatch. Then it is useful the get the shape\nof every intermediate result. This example looks into two\nways of doing it.\n\n## Look into pipeline steps\n\nThe first way is a tricky one: it overloads\nmethods *transform*, *predict* and *predict_proba*\nto keep a copy of inputs and outputs. It then goes\nthrough every step of the pipeline. If the pipeline\nhas *n* steps, it converts the pipeline with step 1,\nthen the pipeline with steps 1, 2, then 1, 2, 3...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy\nfrom onnx.reference import ReferenceEvaluator\nfrom onnxruntime import InferenceSession\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import load_iris\nfrom skl2onnx import to_onnx\nfrom skl2onnx.helpers import collect_intermediate_steps\nfrom skl2onnx.common.data_types import FloatTensorType"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The pipeline.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = load_iris()\nX = data.data\n\npipe = Pipeline(steps=[(\"std\", StandardScaler()), (\"km\", KMeans(3, n_init=3))])\npipe.fit(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The function goes through every step,\noverloads the methods *transform* and\nreturns an ONNX graph for every step.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "steps = collect_intermediate_steps(\n    pipe, \"pipeline\", [(\"X\", FloatTensorType([None, X.shape[1]]))], target_opset=17\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We call method transform to population the\ncache the overloaded methods *transform* keeps.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipe.transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We compute every step and compare\nONNX and scikit-learn outputs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for step in steps:\n    print(\"----------------------------\")\n    print(step[\"model\"])\n    onnx_step = step[\"onnx_step\"]\n    sess = InferenceSession(\n        onnx_step.SerializeToString(), providers=[\"CPUExecutionProvider\"]\n    )\n    onnx_outputs = sess.run(None, {\"X\": X.astype(numpy.float32)})\n    onnx_output = onnx_outputs[-1]\n    skl_outputs = step[\"model\"]._debug.outputs[\"transform\"]\n\n    # comparison\n    diff = numpy.abs(skl_outputs.ravel() - onnx_output.ravel()).max()\n    print(\"difference\", diff)\n\n# That was the first way: dynamically overwrite\n# every method transform or predict in a scikit-learn\n# pipeline to capture the input and output of every step,\n# compare them to the output produced by truncated ONNX\n# graphs built from the first one.\n#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Python runtime to look into every node\n\nThe python runtime may be useful to easily look\ninto every node of the ONNX graph.\nThis option can be used to check when the computation\nfails due to nan values or a dimension mismatch.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "onx = to_onnx(pipe, X[:1].astype(numpy.float32), target_opset=17)\n\noinf = ReferenceEvaluator(onx, verbose=1)\noinf.run(None, {\"X\": X[:2].astype(numpy.float32)})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And to get a sense of the intermediate results.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "oinf = ReferenceEvaluator(onx, verbose=3)\noinf.run(None, {\"X\": X[:2].astype(numpy.float32)})\n\n# This way is usually better if you need to investigate\n# issues within the code of the runtime for an operator."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}