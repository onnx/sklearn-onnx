{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# TfIdfVectorizer with ONNX\n\nThis example is inspired from the following example:\n[Column Transformer with Heterogeneous Data Sources](https://scikit-learn.org/stable/auto_examples/\ncompose/plot_column_transformer.html)\nwhich builds a pipeline to classify text.\n\n## Train a pipeline with TfidfVectorizer\n\nIt replicates the same pipeline taken from *scikit-learn* documentation\nbut reduces it to the part ONNX actually supports without implementing\na custom converter. Let's get the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport os\nfrom onnx.tools.net_drawer import GetPydotGraph, GetOpNodeProducer\nimport onnxruntime as rt\nfrom skl2onnx.common.data_types import StringTensorType\nfrom skl2onnx import convert_sklearn\nimport numpy as np\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.datasets import fetch_20newsgroups\n\ntry:\n    from sklearn.datasets._twenty_newsgroups import (\n        strip_newsgroup_footer,\n        strip_newsgroup_quoting,\n    )\nexcept ImportError:\n    # scikit-learn < 0.24\n    from sklearn.datasets.twenty_newsgroups import (\n        strip_newsgroup_footer,\n        strip_newsgroup_quoting,\n    )\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import classification_report\nfrom sklearn.linear_model import LogisticRegression\n\n\n# limit the list of categories to make running this example faster.\ncategories = [\"alt.atheism\", \"talk.religion.misc\"]\ntrain = fetch_20newsgroups(\n    random_state=1,\n    subset=\"train\",\n    categories=categories,\n)\ntest = fetch_20newsgroups(\n    random_state=1,\n    subset=\"test\",\n    categories=categories,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first transform extract two fields from the data.\nWe take it out form the pipeline and assume\nthe data is defined by two text columns.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class SubjectBodyExtractor(BaseEstimator, TransformerMixin):\n    \"\"\"Extract the subject & body from a usenet post in a single pass.\n    Takes a sequence of strings and produces a dict of sequences. Keys are\n    `subject` and `body`.\n    \"\"\"\n\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, posts):\n        # construct object dtype array with two columns\n        # first column = 'subject' and second column = 'body'\n        features = np.empty(shape=(len(posts), 2), dtype=object)\n        for i, text in enumerate(posts):\n            headers, _, bod = text.partition(\"\\n\\n\")\n            bod = strip_newsgroup_footer(bod)\n            bod = strip_newsgroup_quoting(bod)\n            features[i, 1] = bod\n\n            prefix = \"Subject:\"\n            sub = \"\"\n            for line in headers.split(\"\\n\"):\n                if line.startswith(prefix):\n                    sub = line[len(prefix) :]\n                    break\n            features[i, 0] = sub\n\n        return features\n\n\ntrain_data = SubjectBodyExtractor().fit_transform(train.data)\ntest_data = SubjectBodyExtractor().fit_transform(test.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The pipeline is almost the same except\nwe remove the custom features.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipeline = Pipeline(\n    [\n        (\n            \"union\",\n            ColumnTransformer(\n                [\n                    (\"subject\", TfidfVectorizer(min_df=50, max_features=500), 0),\n                    (\n                        \"body_bow\",\n                        Pipeline(\n                            [\n                                (\"tfidf\", TfidfVectorizer()),\n                                (\"best\", TruncatedSVD(n_components=50)),\n                            ]\n                        ),\n                        1,\n                    ),\n                    # Removed from the original example as\n                    # it requires a custom converter.\n                    # ('body_stats', Pipeline([\n                    #   ('stats', TextStats()),  # returns a list of dicts\n                    #   ('vect', DictVectorizer()),  # list of dicts -> feature matrix\n                    # ]), 1),\n                ],\n                transformer_weights={\n                    \"subject\": 0.8,\n                    \"body_bow\": 0.5,\n                    # 'body_stats': 1.0,\n                },\n            ),\n        ),\n        # Use a LogisticRegression classifier on the combined features.\n        # Instead of LinearSVC (not fully ready in onnxruntime).\n        (\"logreg\", LogisticRegression()),\n    ]\n)\n\npipeline.fit(train_data, train.target)\nprint(classification_report(pipeline.predict(test_data), test.target))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ONNX conversion\n\nIt is difficult to replicate the exact same tokenizer\nbehaviour if the tokeniser comes from space, gensim or nltk.\nThe default one used by *scikit-learn* uses regular expressions\nand is currently being implementing. The current implementation\nonly considers a list of separators which can is defined\nin variable *seps*.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "seps = {\n    TfidfVectorizer: {\n        \"separators\": [\n            \" \",\n            \".\",\n            \"\\\\?\",\n            \",\",\n            \";\",\n            \":\",\n            \"!\",\n            \"\\\\(\",\n            \"\\\\)\",\n            \"\\n\",\n            '\"',\n            \"'\",\n            \"-\",\n            \"\\\\[\",\n            \"\\\\]\",\n            \"@\",\n        ]\n    }\n}\nmodel_onnx = convert_sklearn(\n    pipeline,\n    \"tfidf\",\n    initial_types=[(\"input\", StringTensorType([None, 2]))],\n    options=seps,\n    target_opset=12,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And save.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with open(\"pipeline_tfidf.onnx\", \"wb\") as f:\n    f.write(model_onnx.SerializeToString())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Predictions with onnxruntime.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sess = rt.InferenceSession(\"pipeline_tfidf.onnx\", providers=[\"CPUExecutionProvider\"])\nprint(\"---\", train_data[0])\ninputs = {\"input\": train_data[:1]}\npred_onx = sess.run(None, inputs)\nprint(\"predict\", pred_onx[0])\nprint(\"predict_proba\", pred_onx[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With *scikit-learn*:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(pipeline.predict(train_data[:1]))\nprint(pipeline.predict_proba(train_data[:1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are discrepencies for this model because\nthe tokenization is not exactly the same.\nThis is a work in progress.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Display the ONNX graph\n\nFinally, let's see the graph converted with *sklearn-onnx*.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pydot_graph = GetPydotGraph(\n    model_onnx.graph,\n    name=model_onnx.graph.name,\n    rankdir=\"TB\",\n    node_producer=GetOpNodeProducer(\n        \"docstring\", color=\"yellow\", fillcolor=\"yellow\", style=\"filled\"\n    ),\n)\npydot_graph.write_dot(\"pipeline_tfidf.dot\")\n\nos.system(\"dot -O -Gdpi=300 -Tpng pipeline_tfidf.dot\")\n\nimage = plt.imread(\"pipeline_tfidf.dot.png\")\nfig, ax = plt.subplots(figsize=(40, 20))\nax.imshow(image)\nax.axis(\"off\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}