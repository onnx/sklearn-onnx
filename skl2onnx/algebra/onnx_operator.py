# -------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See License.txt in the project root for
# license information.
# --------------------------------------------------------------------------
import numpy as np
import onnx
from ..common.data_types import FloatTensorType, Int64TensorType, StringTensorType
from ..common._topology import Variable
from .graph_state import GraphState
from .symbolic_op_tranformer import SymbolicOpTransformer


class OnnxOperator:
    """
    Ancestor to every *ONNX* operator exposed in
    :mod:`onnx_ops` and :mod:`onnx_ops_ml`. These files
    are automatically generated by unit test
    *test_onnx_operators_parse_spec*
    Every instance is supposed to be included in
    a graph as a node.

    :param inputs: list of inputs expected by the operator
    :param outputs: empty if this node is not a final node,
        its outputs are not the graph outputs
    :param kwargs: additional parameters of the operator
    """

    def __init__(self, *inputs, outputs=None, **kwargs):
        self.state = None
        self.inputs = list(inputs)
        self.kwargs = kwargs
        if outputs is None:
            # It means intermediate outputs. We suppose there is one.
            outputs = [None]
        self.known_outputs = outputs

    def _register_outputs(self, scope):
        """
        Registers outputs in *scope* if not alreay registered.
        It should not be directly called but from method *add_to*.

        :param scope: scope
        """
        for i in range(len(self.known_outputs)):
            o = self.known_outputs[i]
            if o is None:
                self.known_outputs[i] = scope.get_unique_variable_name(
                    self.__class__.__name__ + '-o')

    def add_to(self, scope, operator, container):
        """
        Adds outputs to the container if not already added,
        registered the outputs if the node is not final.

        :param scope: scope
        :param operator: operator
        :param container: container
        """
        if self.state is None:
            if self.kwargs.get('op_version', '') is None:
                kwargs = self.kwargs.copy()
                del kwargs['op_version']
            else:
                kwargs = self.kwargs

            if operator is not None and len(self.known_outputs) \
                    == 1 and self.known_outputs[0] is None:
                self.known_outputs = operator.outputs

            self._register_outputs(scope)
            self.state = GraphState(self.inputs, self.known_outputs,
                                    self.__class__.__name__,
                                    scope, container, None,
                                    **self.kwargs)
            self.state.run()

    @property
    def outputs(self):
        """
        Returns the outputs of the node.
        """
        if self.state is None:
            raise RuntimeError("Method add was not called.")
        return self.state.outputs

    def check_inputs(self, inputs):
        """
        Checks the given inputs follow the constraints
        defined by the operator.
        """
        if isinstance(inputs, list):
            inputs = {v.onnx_name: v for v in inputs}
        if not isinstance(inputs, dict):
            raise TypeError(
                "inputs must be a dictionary, not a {}".format(type(inputs)))
        if self.input_range is not None:
            if len(inputs) < self.input_range[0] or \
                    len(inputs) > self.input_range[1]:
                raise RuntimeError("Operator '{}' expects a number of inputs "
                                   "in [{}, {}] not {}".format(
                                       self.__class__.__name__, *self.input_range,
                                       len(inputs)))
        res = []
        for k, value in inputs.items():
            exp = [v for v in self.expected_inputs if v[0] == k]
            if len(exp) == 0:
                raise RuntimeError("Operator has not input '{}', "
                                   "expects a name in {}.".format(
                                       k, [v[0] for v in self.expected_inputs]))
            exp = exp[0]
            if hasattr(exp[1], 'name') and exp[1].name != exp[0]:
                raise RuntimeError("Name mismatch '{}' != '{}'".format(
                    exp[1].name, exp[0]))
            res.append((exp[0], self.guess_type(exp[1], value)))
        return res

    def get_expected_outputs(self, inputs):
        """
        Infers the number of outputs given the inputs.
        """
        if self.output_range[0] == self.output_range[1]:
            nb = self.output_range[0]
        else:
            nb = min(
                max(len(inputs), self.output_range[0]), self.output_range[1])
        res = []
        if inputs:
            for i in range(nb):
                exp = self.expected_outputs[i]
                inp = inputs[i] if i < len(inputs) else None
                if exp[1]:
                    res.append(Variable(exp[0], exp[0], None, exp[1]))
                elif inp:
                    res.append(Variable(exp[0], exp[0], None, inp.type))
                else:
                    res.append(Variable(exp[0], exp[0]))
        else:
            for i in range(nb):
                exp = self.expected_outputs[i]
                if exp[1]:
                    res.append(Variable(exp[0], exp[0], None, exp[1]))
                else:
                    res.append(Variable(exp[0], exp[0]))
        return res

    def guess_type(self, expected_type, given_type):
        """
        Returns the proper type of an input.
        """
        if isinstance(given_type, np.ndarray):
            if given_type.dtype == np.float32:
                return FloatTensorType(given_type.shape)
            elif given_type.dtype == np.int64:
                return Int64TensorType(given_type.shape)
            elif given_type.dtype == np.str:
                return StringTensorType(given_type.shape)
            else:
                raise NotImplementedError(
                    "Unsupported type '{}'".format(given_type.dtype))
        elif isinstance(given_type, (FloatTensorType, Int64TensorType, StringTensorType)):
            return given_type
        elif isinstance(given_type, Variable):
            return given_type.type
        elif isinstance(given_type, onnx.onnx_ml_pb2.TensorProto):
            if given_type.data_type == 1:
                return FloatTensorType(given_type.dims)
            elif given_type.data_type == 8:
                return StringTensorType(given_type.dims)
            elif given_type.data_type == 7:
                return Int64TensorType(given_type.dims)
            else:
                raise NotImplementedError("Unsupported type '{}' data_type={}".format(
                    type(given_type), given_type.data_type))
        else:
            raise NotImplementedError(
                "Unsupported type '{}'".format(type(given_type)))

    def to_onnx(self, inputs=None):
        """
        Converts this operator into an ONNX graph.

        :param inputs: specific inputs (as a dictionary) or default inputs if not specified
        """
        from .. import convert_sklearn
        if inputs:
            inputs = self.check_inputs(inputs)
        else:
            inputs = self.expected_inputs
        for name, typ in inputs:
            if typ in (None, ''):
                raise RuntimeError("Type input '{}' for operator '{}' "
                                   "is unknown. You should specify input types.".format(
                                       name, self.__class__.__name__))
        new_cl = type('SymbolicOpTransformer' + self.__class__.__name__,
                      (SymbolicOpTransformer,), {})
        tr = new_cl(self)
        onx = convert_sklearn(tr, 'onnx-op', inputs,
                              custom_parsers={new_cl: tr.parse},
                              custom_shape_calculators={new_cl: tr.set_shape},
                              custom_conversion_functions={new_cl: tr.convert})
        return onx
